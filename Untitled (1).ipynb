{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f86139d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f86139d",
        "outputId": "6a7cbe46-4839-43e9-9650-6b581a8f3040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import SnowballStemmer\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# imports for modelling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    ExtraTreesClassifier,\n",
        ")\n",
        "from sklearn.svm import LinearSVC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')\n",
        "\n",
        "# Loading test_set.csv and train_set.csv\n",
        "df_test = pd.read_csv('test_set.csv')\n",
        "df_train = pd.read_csv('train_set.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwAwgDQIvydy",
        "outputId": "afa94ab9-1a87-48d3-eabf-d5eda280cffe"
      },
      "id": "xwAwgDQIvydy",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ON1t0Q6qs8Xo"
      },
      "id": "ON1t0Q6qs8Xo"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "da33da9a",
      "metadata": {
        "id": "da33da9a"
      },
      "outputs": [],
      "source": [
        "  # Reading the test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IacbOHr4tQhr"
      },
      "id": "IacbOHr4tQhr",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca32b3ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca32b3ab",
        "outputId": "e3d5f69d-39c3-4654-c035-ad9cfdbd1a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  lang_id                                               text\n",
            "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
            "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
            "2     eng  the province of kwazulu-natal department of tr...\n",
            "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
            "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana...\n",
            "   index                                               text\n",
            "0      1  Mmasepala, fa maemo a a kgethegileng a letlele...\n",
            "1      2  Uzakwaziswa ngokufaneleko nakungafuneka eminye...\n",
            "2      3         Tshivhumbeo tshi fana na ngano dza vhathu.\n",
            "3      4  Kube inja nelikati betingevakala kutsi titsini...\n",
            "4      5                      Winste op buitelandse valuta.\n"
          ]
        }
      ],
      "source": [
        "print(df_train.head())\n",
        "print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d822d7fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d822d7fb",
        "outputId": "d1b33986-1160-4e26-af27-835028ee28a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['lang_id', 'text'], dtype='object')\n",
            "Index(['index', 'text'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(df_train.columns)\n",
        "\n",
        "print(df_test.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d5370897",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5370897",
        "outputId": "bd87c818-3488-4d8e-a892-ef1682415385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization and lowercasing\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)  # Convert tokens back to text\n",
        "\n",
        "# Applying  preprocessing to 'text' column in train and test DataFrames\n",
        "df_train['processed_text'] = df_train['text'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8c4c13da",
      "metadata": {
        "id": "8c4c13da"
      },
      "outputs": [],
      "source": [
        "# Applying preprocessing to 'text' column in train and test DataFrames\n",
        "df_train['processed_text'] = df_train['text'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "48fec288",
      "metadata": {
        "id": "48fec288"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Defintion of  X and y\n",
        "X = df_train['processed_text']  # Features\n",
        "y = df_train['lang_id']  # Target\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()  #\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d7fbcd7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "d7fbcd7f",
        "outputId": "ca41508b-ac6a-492d-a2c6-30ee71270c9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression model\n",
        "logistic_model = LogisticRegression(max_iter=1000)  #\n",
        "\n",
        "# Training the model on the training data\n",
        "logistic_model.fit(X_train, y_train)  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "99e895c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99e895c2",
        "outputId": "4aa40aa9-cabd-425b-f1b5-be86f7f419ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9940909090909091\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prediction on the validation set\n",
        "val_predictions = logistic_model.predict(X_val)\n",
        "# Evaluation on  model performance\n",
        "accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f\"Validation Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c8864c83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8864c83",
        "outputId": "bfe37e19-4952-4e07-c2c9-f6cfb16987dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9940909090909091\n"
          ]
        }
      ],
      "source": [
        "print(f\"Validation Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "508b0271",
      "metadata": {
        "id": "508b0271"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_test = df_test['text']\n",
        "\n",
        "# same Vectorizer for training\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# predictions of the vectorized test data\n",
        "test_predictions = logistic_model.predict(X_test_vectorized)\n",
        "\n",
        "# predictions to test dataframe\n",
        "df_test['predicted_language'] = test_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "512cd0eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "512cd0eb",
        "outputId": "0c24815f-5bcb-45c4-fe72-ebe5bcd912cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['index', 'text', 'processed_text', 'predicted_language'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df_test.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ed00d271",
      "metadata": {
        "id": "ed00d271"
      },
      "outputs": [],
      "source": [
        "submission = df_test[['index', 'predicted_language']]  #\n",
        "# Rename 'predicted_language' to 'lang_id'\n",
        "submission = submission.rename(columns={'predicted_language': 'lang_id'})\n",
        "\n",
        "# Save submission to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)  # Adjust file name as needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# file downlod\n",
        "files.download('submission.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aQTC38v282Nb",
        "outputId": "c0aa21c7-d002-4543-df97-f57379a7ce8a"
      },
      "id": "aQTC38v282Nb",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64198a57-531b-44a0-89b8-82aeb5b91a76\", \"submission.csv\", 50045)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}